# ğŸ¤– Pro RAG Chatbot

A powerful **Retrieval-Augmented Generation (RAG)** chatbot that lets you chat with your documents locally and privately. Upload PDFs, text files, or markdown â€” and get accurate, context-aware answers powered by local LLMs via [Ollama](https://ollama.com/).

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-FF4B4B?logo=streamlit&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-black?logo=ollama&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-Framework-green)
![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-yellow)
![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)

---

## âœ¨ Features

### ğŸ”’ Core
- **Fully Local & Private** â€” Your documents never leave your machine. No cloud APIs required.
- **Multi-Format Support** â€” Upload and chat with PDFs, `.txt`, and `.md` files.
- **ğŸŒ URL Ingestion** â€” Paste any web URL to fetch and index the page content.
- **âš¡ Streaming Responses** â€” Real-time token-by-token output for a smooth chat experience.
- **ğŸ–¥ï¸ CLI Mode** â€” Prefer the terminal? Use `cli.py` for a command-line chat interface.

### ğŸ§  Intelligence
- **ğŸ’¬ Conversation Memory** â€” Follow-up questions work naturally with 6-turn context window.
- **ğŸ” Relevance Scores** â€” Color-coded confidence badges (ğŸŸ¢ high / ğŸŸ¡ mid / ğŸ”´ low) on every source.
- **ğŸ¯ Focus Mode** â€” Lock the AI's attention to a single document for precise answers.
- **âœï¸ Custom System Prompt** â€” Tune the AI's behavior and personality from the sidebar.
- **âš¡ Quick Prompts** â€” One-click starter suggestions (Summarize, Key Points, Author Info, etc.).

### ğŸ¨ Experience
- **ğŸ¨ 5 Color Themes** â€” Midnight Purple, Ocean Blue, Emerald, Sunset, Rose Gold.
- **ğŸ“Š Analytics Dashboard** â€” Live stats: documents, pages, chunks, queries, avg response time, tokens.
- **â±ï¸ Response Metrics** â€” Per-answer timing, token count, and tokens/second.
- **ğŸ”Š Text-to-Speech** â€” Click "Read Aloud" to hear any answer spoken by the browser.
- **ğŸ’¬ Chat Sessions** â€” Save, name, and switch between multiple conversations.
- **ğŸ’¾ Export Chat** â€” Download your conversation as a Markdown file.

### ğŸ› ï¸ Model Management
- **ğŸ§  Dynamic Model Switching** â€” Switch between any locally installed Ollama model instantly.
- **ğŸ“¥ Download Models from UI** â€” Pull new Ollama models directly from the sidebar.
- **ğŸ”§ Configurable** â€” Tune temperature, search depth, chunk size, and more via `.env` or the UI.

---

## ğŸ“ Project Structure

```
RAGLLM/
â”œâ”€â”€ app.py              # Streamlit web interface
â”œâ”€â”€ cli.py              # Terminal-based chat interface
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ .env                # Environment variables (create this)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py       # All settings & defaults
â”‚   â”œâ”€â”€ utils.py        # Embeddings, LLM, FAISS, Ollama helpers
â”‚   â”œâ”€â”€ ingestion.py    # Document loading, chunking, indexing
â”‚   â””â”€â”€ core.py         # RAG logic â€” retriever, prompts, generation
â”œâ”€â”€ data/               # Uploaded documents (auto-created)
â””â”€â”€ vector_index/       # FAISS index storage (auto-created)
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **[Ollama](https://ollama.com/download)** installed and running
- At least one Ollama model pulled (e.g., `ollama pull llama3.2:3b`)

### Installation

```bash
# Clone the repo
git clone https://github.com/ShibilAhamed701212/RAGLLM.git
cd RAGLLM

# Create virtual environment
python -m venv venv

# Activate it
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration (Optional)

Create a `.env` file in the project root to customize settings:

```env
# LLM Provider: "ollama" (default) or "openai"
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2:3b

# OpenAI (only if LLM_PROVIDER=openai)
OPENAI_API_KEY=your-key-here
OPENAI_MODEL=gpt-4o-mini

# Embedding model
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# Retrieval settings
TOP_K=5
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
DEFAULT_TEMPERATURE=0.1
SEARCH_TYPE=similarity
```

### Run the App

```bash
# Web UI (recommended)
streamlit run app.py

# CLI mode
python cli.py
```

---

## ğŸ§  Supported Ollama Models

You can use **any model** from the [Ollama Library](https://ollama.com/library). Here are some recommended ones:

| Model | Size | Best For |
|---|---|---|
| `llama3.2:1b` | 1.3 GB | Fast responses, low RAM |
| `llama3.2:3b` | 2.0 GB | Good balance (default) |
| `llama3:8b` | 4.7 GB | High quality answers |
| `gemma3:4b` | 3.3 GB | Strong reasoning |
| `phi4-mini` | 2.5 GB | Efficient & capable |
| `mistral` | 4.1 GB | Great all-rounder |
| `qwen3:4b` | 2.5 GB | Multilingual support |
| `deepseek-r1:8b` | 4.9 GB | Advanced reasoning |

**Download any model** directly from the app sidebar or via terminal:
```bash
ollama pull gemma3:4b
```

---

## ğŸ”§ How It Works

1. **Upload** â€” Drop your PDFs, text files, or markdown into the app.
2. **Ingest** â€” Documents are chunked, embedded using `bge-small-en-v1.5`, and stored in a FAISS vector index.
3. **Query** â€” Your question is embedded and matched against the most relevant document chunks.
4. **Generate** â€” The retrieved context + your question are sent to the local LLM, which generates a grounded answer.

```
User Question â†’ Embed â†’ FAISS Search â†’ Top-K Chunks â†’ LLM â†’ Answer
```

---

## ğŸ›¡ï¸ Privacy

This project is designed for **complete privacy**:
- All processing happens **locally** on your machine.
- No data is sent to external servers (when using Ollama).
- Documents are stored in a local `data/` directory.
- Vector embeddings are stored in a local `vector_index/` directory.

---

## ğŸ“ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests.

---

<p align="center">
  Made with â¤ï¸ by <a href="https://github.com/ShibilAhamed701212">Shibil Ahamed</a>
</p>
